{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "决策树\n",
    "优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据\n",
    "缺点：可能产生过度匹配\n",
    "适用数据类型：数值型和标称型\n",
    "一般流程：\n",
    "1、收集数据：可以使用任何方法\n",
    "2、准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化\n",
    "3、分析数据：检查在构造的树后的图形是否符合预期\n",
    "4、训练算法：构造树的数据结构\n",
    "5、测试算法：计算错误率\n",
    "6、使用算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1、决策树的构造\n",
    "1.1\n",
    "   第一个问题：当前数据集哪个特征在划分数据分类时起关键作用？\n",
    "   为了找到决定性的特征，划分最好的结果，我们必须评估每个特征，完成测试后原始数据被划分成了几个数据子集，这些子集分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，那么无需继续进行分割，否则继续分割。\n",
    "   伪代码：\n",
    "   检测数据集中每个子项是否属于同一分类：\n",
    "     if so return 类标签\n",
    "     else\n",
    "       寻找划分数据集合的最好特征\n",
    "       划分数据集\n",
    "       创建分直接点\n",
    "       for each子集\n",
    "         递归调用本函数返回结果到分支节点中\n",
    "      return 分支节点\n",
    "1.2\n",
    "   如何划分数据找到决定性的特征呢？\n",
    "   划分数据集的大原则是：将无序的数据变的更加有序\n",
    "   组织杂乱无章的数据的一种方法就是使用信息论度量信息，信息论是量化处理信息的分支科学，在划分数据集前后发生的变化称为信息增益，获得信息增益最高的特征就是最好的选择。\n",
    "   熵定义为信息的期望值，在此之前我们需要知道信息的定义：如果待分类的事务可能划分在多个分类中，则符号x的信息定义为l(x)=-log2 p(x) 其中p(x)是选择该分类的概率\n",
    "   为了计算熵，我们需要将计算的所有类别所有可能值包含的信息期望值，H=-Σp(x) log2 p(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算给定数据集的熵\n",
    "from math import log\n",
    "def calEnt(dataset):\n",
    "    numEntries=len(dataset)\n",
    "    labelCount={}\n",
    "    for feaVec in dataset:\n",
    "        curlabel=feaVec[-1]\n",
    "        if curlabel not in labelCount.keys():\n",
    "            labelCount[curlabel]=0\n",
    "        labelCount[curlabel]+=1\n",
    "    Ent=0.0\n",
    "    for key in labelCount:\n",
    "        p=float(labelCount[key]/numEntries)\n",
    "        Ent-=p*log(p,2)\n",
    "    return Ent\n",
    "#测试\n",
    "def createdataset():\n",
    "    dataset=[[1,1,'yes'],\n",
    "             [1,1,'yes'],\n",
    "             [1,0,'no'],\n",
    "             [0,1,'no'],\n",
    "             [0,1,'no']\n",
    "            ]\n",
    "    labels=['no sufacing','filippers']\n",
    "    return dataset,labels\n",
    "mydata,mylabel=createdataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   上面我们学习了如何度量数据集的无序程度，分类算法除了需要测量信息熵，还需要划分数据集，度量划分数据集合的熵，以便判断当前是否正确划分了数据集，我们将对每个特征划分数据集的结果计算一次信息熵，然后判断那个特征划分是最好的划分方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据数据的特征以及每个特征不同取值划分数据集\n",
    "def splitdatabyfeature(dataset,index,value):\n",
    "    restdataset=[]\n",
    "    for feature in dataset:\n",
    "        if feature[index]==value:\n",
    "            reducedfeature=feature[:index]\n",
    "            reducedfeature.extend(feature[index+1:])\n",
    "            restdataset.append(reducedfeature)\n",
    "    return restdataset\n",
    "\n",
    "\n",
    "#print(splitdatabyfeature(mydata,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算数据的每个特征的信息增益\n",
    "#基本步骤：\n",
    "#1、计算当前熵值\n",
    "#2、计算每个特征对应的信息增益\n",
    "#   2.1根据一个特征的不同取值划分数据集\n",
    "#   2.2根据不同取值value在特征中所占比例，计算熵值\n",
    "#   2.3计算信息增益 Ent(D)-Σ（prob *Ent（DV））\n",
    "#3、记录并返回最大信息增益对应的特征\n",
    "def chooseBestFeatureTosplit(dataset):\n",
    "    numfeature=len(dataset[0])-1\n",
    "    bestinfogain=0.0\n",
    "    bestfeature=-1\n",
    "    Ent=calEnt(dataset)\n",
    "    for i in range(numfeature):\n",
    "        category=[example[i] for example in dataset]\n",
    "        category=set(category)#去重                             \n",
    "        #print(category)\n",
    "        newEnt=0.0\n",
    "        for value in category:\n",
    "            restdata=splitdatabyfeature(dataset,i,value)\n",
    "            prob=len(restdata)/float(len(dataset))\n",
    "            newEnt+=prob*calEnt(restdata)\n",
    "        infogain=Ent-newEnt\n",
    "        if infogain>bestinfogain:\n",
    "            bestinfogain=infogain\n",
    "            bestfeature=i\n",
    "    return bestfeature   \n",
    "\n",
    "#print(chooseBestFeatureTosplit(mydata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "def majorityCnt(classname):\n",
    "    classcount={}\n",
    "    for vote in classname:\n",
    "        if vote not in classcount.keys():\n",
    "            classcount[vote]=0\n",
    "        classcount[vote]+=1\n",
    "    sortedclasscount=sorted(classcount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedclasscount[0][0]\n",
    "majorityCnt(['yes','yes','no','no','no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createtree(dataset,labels):\n",
    "    classname=[example[-1] for example in dataset]\n",
    "    if classname.count(classname[0])==len(classname):\n",
    "        return classname[0]\n",
    "    if len(dataset[0])==1:\n",
    "        return majorityCnt(classname)\n",
    "    selectedfeature=chooseBestFeatureTosplit(dataset)\n",
    "    #print(labels)\n",
    "    selectedlabel=labels[selectedfeature]\n",
    "    value=[example[selectedfeature] for example in dataset]\n",
    "    value=set(value)\n",
    "    mytree={selectedlabel:{}}\n",
    "    del(labels[selectedfeature])\n",
    "    for eachvalue in value:\n",
    "        currdataset=splitdatabyfeature(dataset,selectedfeature,eachvalue)\n",
    "        currlabels=labels[:]\n",
    "        mytree[selectedlabel][eachvalue]=createtree(currdataset,currlabels)\n",
    "    return mytree\n",
    "label=mylabel[:]\n",
    "mytree=createtree(mydata,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用决策树进行分类\n",
    "def classify(inputtree,labels_,features):\n",
    "    FeatureName=list(inputtree.keys())[0]\n",
    "    FeatureValue=inputtree[FeatureName]\n",
    "    #print(labels_)\n",
    "    FeatureIndex=labels_.index(FeatureName)\n",
    "    SecondDict=inputtree[FeatureName]    #二级字典\n",
    "    for value in FeatureValue:\n",
    "        if features[FeatureIndex]==value:\n",
    "            if type(SecondDict[value]).__name__=='dict':\n",
    "                classlabel=classify(SecondDict[value],labels_,features)\n",
    "            else:\n",
    "                classlabel=SecondDict[value]\n",
    "    return classlabel\n",
    "\n",
    "classify(mytree,mylabel,[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no sufacing': {0: 'no', 1: {'filippers': {0: 'no', 1: 'yes'}}}}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在计算机中存储决策树分类器pickle序列化对象\n",
    "def storetree(inputtree,filename):\n",
    "    import pickle\n",
    "    fw=open(filename,'wb')\n",
    "    pickle.dump(inputtree,fw)\n",
    "    fw.close()\n",
    "def gettree(filename):\n",
    "    import pickle\n",
    "    fr =open(filename,'rb')\n",
    "    return pickle.load(fr)\n",
    "\n",
    "storetree(mytree,'classifystorage.txt')\n",
    "gettree('classifystorage.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tearrate': {'normal': {'astigmatic': {'no': {'age': {'pre': 'soft',\n",
       "      'presbyopic': {'prescript': {'hyper': 'soft', 'myope': 'no lenses'}},\n",
       "      'young': 'soft'}},\n",
       "    'yes': {'prescript': {'hyper': {'age': {'pre': 'no lenses',\n",
       "        'presbyopic': 'no lenses',\n",
       "        'young': 'hard'}},\n",
       "      'myope': 'hard'}}}},\n",
       "  'reduced': 'no lenses'}}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#隐形眼镜类型分类\n",
    "fr=open('data.txt')\n",
    "lenses=[inst.strip().split('\\t') for inst in fr.readlines()]\n",
    "lesessLabels=['age','prescript','astigmatic','tearrate']\n",
    "lensetree=createtree(lenses,lesessLabels)\n",
    "lensetree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
