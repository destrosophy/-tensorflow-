{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MyDownloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\MyDownloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\MyDownloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\MyDownloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\MyDownloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\MyDownloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already!\n",
      "already well\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import tarfile\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from random import randint\n",
    "url=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "filepath='aclImdb_v1.tar.gz'\n",
    "if not os.path.isfile(filepath):\n",
    "    print(\"download--\")\n",
    "    result=urllib.request.urlretrieve(url,filepath)\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"already!\")\n",
    "if not os.path.exists(\"aclImdb\"):\n",
    "    tfile=tarfile.open(filepath,\"r:gz\")\n",
    "    print(\"extracting...\")\n",
    "    result=tfile.extractall()\n",
    "else:\n",
    "    print(\"already well\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "        re_tag=re.compile(r'<[^>]+>')\n",
    "        return re_tag.sub('',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filetype):\n",
    "        path=\"/aclImdb/\"\n",
    "        file_list=[]\n",
    "        positive_path=path+filetype+\"/pos/\"\n",
    "        for f in os.listdir(positive_path):\n",
    "            file_list+=[positive_path+f]\n",
    "        pos_files_num=len(file_list)\n",
    "        \n",
    "        negative_path=path+filetype+\"/neg/\"\n",
    "        for f in os.listdir(negative_path):\n",
    "            file_list+=[negative_path+f]\n",
    "        neg_files_num=len(file_list)-pos_files_num\n",
    "        #print(file_list)\n",
    "        all_labels=([[1,0]]*pos_files_num+[[0,1]]*neg_files_num)\n",
    "        all_text=[]\n",
    "        for fi in file_list:\n",
    "            with open(fi,encoding='utf8') as file_input:\n",
    "                all_text+=[remove_tags(\" \".join(file_input.readlines()))]\n",
    "        return all_labels,all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts,train_labels=read_files(\"train\")\n",
    "test_texts,test_labels=read_files(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#建立词汇词典tokSen\n",
    "#! pip install keras==2.0.6\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "token=Tokenizer(num_words=4000)\n",
    "token.fit_on_texts(train_labels)\n",
    "y_train=train_texts\n",
    "y_test=test_texts\n",
    "train_sequence=token.texts_to_sequences(train_labels)\n",
    "test_sequence=token.texts_to_sequences(test_labels)\n",
    "x_train=keras.preprocessing.sequence.pad_sequences(train_sequence,\n",
    "                                                  padding='post',\n",
    "                                                  truncating='post',\n",
    "                                                  maxlen=400)\n",
    "x_test=keras.preprocessing.sequence.pad_sequences(test_sequence,\n",
    "                                                  padding='post',\n",
    "                                                  truncating='post',\n",
    "                                                  maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 73s - loss: 0.6067 - acc: 0.6734 - val_loss: 0.6280 - val_acc: 0.7510\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 80s - loss: 0.4068 - acc: 0.8366 - val_loss: 0.8752 - val_acc: 0.6364\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 79s - loss: 0.3467 - acc: 0.8672 - val_loss: 0.6768 - val_acc: 0.7120\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 80s - loss: 0.3237 - acc: 0.8770 - val_loss: 0.4478 - val_acc: 0.8248\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 79s - loss: 0.2829 - acc: 0.8985 - val_loss: 0.5222 - val_acc: 0.7922\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(output_dim=32,\n",
    "                                input_dim=4000,\n",
    "                                input_length=400))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=8)))\n",
    "model.add(keras.layers.Dense(units=32,activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(units=2,activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "hist=model.fit(x_train,y_train,validation_split=0.2,epochs=5,batch_size=128,verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
